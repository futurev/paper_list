# vision_language

[Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/pdf/2302.14045.pdf)

[]()
[]()
[]()
[]()

# DETR Series
[DETR, End-to-End Object Detection with Transformers](https://arxiv.org/pdf/2005.12872)

[Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://arxiv.org/abs/2010.04159)

[DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection](https://arxiv.org/abs/2203.03605)

[DN-DETR: Accelerate DETR Training by Introducing Query DeNoising](https://arxiv.org/abs/2203.01305)

[DETA, NMS strikes back](https://arxiv.org/abs/2212.06137)



# video & tracking
[Divert More Attention to Vision-Language Tracking](https://arxiv.org/pdf/2207.01076.pdf)

[Towards More Flexible and Accurate Object Tracking with Natural Language: Algorithms and Benchmark](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Towards_More_Flexible_and_Accurate_Object_Tracking_With_Natural_Language_CVPR_2021_paper.pdf) CVPR2021, 

[Real-time Visual Object Tracking with Natural Language Description](https://openaccess.thecvf.com/content_WACV_2020/papers/Feng_Real-time_Visual_Object_Tracking_with_Natural_Language_Description_WACV_2020_paper.pdf)

[Exploiting Unlabeled Data with Vision and Language Models for Object Detection](https://arxiv.org/pdf/2207.08954)


## SSL+Pre-training

[]()
[ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf), NIPS2019, Facebook
[VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) UCLA

[A Survey of Vision-Language Pre-Trained Models](https://arxiv.org/pdf/2202.10936)


[Towards a Unified Foundation Model: Jointly Pre-Training Transformers on Unpaired Images and Text](https://arxiv.org/abs/2112.07074), Google.

[ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation](https://arxiv.org/abs/2112.15283), Baidu.

*[FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/pdf/2112.04482.pdf), Meta AI.

[A Fistful of Words: Learning Transferable Visual Models from Bag-of-Words Supervision](https://arxiv.org/abs/2112.13884), Meta AI.

*[SLIP: Self-supervision meets Language-Image Pre-training](https://arxiv.org/abs/2112.12750), [[code](https://github.com/facebookresearch/SLIP)], Meta AI.

*[MLIM: Vision-and-Language Model Pre-training with Masked Language and Image Modeling](https://arxiv.org/abs/2109.12178), Amazon.

*[Data Efficient Masked Language Modeling for Vision and Language](https://arxiv.org/abs/2109.02040), [[code](https://github.com/yonatanbitton/data_efficient_masked_language_modeling_for_vision_and_language)], Ben Gurion University.

[VL-BERT: Pre-training of Generic Visual-Linguistic Representations](https://arxiv.org/abs/1908.08530), [[code](https://github.com/jackroos/VL-BERT)], MSRA.

[Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training](https://arxiv.org/pdf/1908.06066.pdf), [[code](https://github.com/microsoft/Unicoder)], MSRA.

[UNITER: UNiversal Image-TExt Representation Learning](https://arxiv.org/abs/1909.11740), [[code](https://github.com/ChenRocks/UNITER)], Microsoft.

[ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](https://arxiv.org/abs/1908.02265), [[code](https://github.com/facebookresearch/vilbert-multi-task)], Meta AI.


## Masked Image Modeling

[Masked Feature Prediction for Self-Supervised Visual Pre-Training](https://arxiv.org/abs/2112.09133), Meta AI.

[SimMIM: A Simple Framework for Masked Image Modeling](https://arxiv.org/abs/2111.09886), [[code](https://github.com/microsoft/SimMIM)], MSRA.

[PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers](https://arxiv.org/abs/2111.12710), [[code](https://github.com/microsoft/PeCo)], MSRA.

[Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/pdf/2111.06377.pdf), [[Unofficial code](https://github.com/pengzhiliang/MAE-pytorch)], Meta AI.

[iBOT: Image BERT Pre-Training with Online Tokenizer](https://arxiv.org/abs/2111.07832), [[code](https://github.com/bytedance/ibot)], ByteDance .

[BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254), [[code](https://github.com/microsoft/unilm)], MSRA.

## Masked Language Modeling
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805), [[code](https://github.com/google-research/bert)], Google.

## Others

[Align and Prompt: Video-and-Language Pre-training with Entity Prompts](https://arxiv.org/abs/2112.09583), Saleforce.

[FILIP: Fine-grained Interactive Language-Image Pre-Training](https://arxiv.org/pdf/2111.07783.pdf), [[Unofficial code](https://github.com/lucidrains/x-clip)], Huawei.

[LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991), [[Unofficial code](https://github.com/lucidrains/x-clip)], Google.

[Multimodal Few-Shot Learning with Frozen Language Models](https://proceedings.neurips.cc/paper/2021/file/01b7575c38dac42f3cfb7d500438b875-Paper.pdf), DeepMind.

[Prompting Visual-Language Models for Efficient Video Understanding](https://arxiv.org/pdf/2112.04478.pdf), [[code](https://github.com/ju-chen/Efficient-Prompt)], SJTU.

[TiP-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling](https://arxiv.org/abs/2111.03930), [[code](https://github.com/gaopengcuhk/Tip-Adapter)], CUHK.
